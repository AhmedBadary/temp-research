---
layout: NotesPage
title: 2.4 <br /> Error Analysis For Iterative Methods 
permalink: /work_files/school/128a/2_4
prevLink: /work_files/school/2
---

<div markdown="1" class = "TOC">
# Table of Contents

  * [Order of Convergence ](#content1)
  {: .TOC1}
  * [Multiple Roots ](#content2)
  {: .TOC2}
</div>



***
***


## Order of Convergence 
{: #content1}

1. **Order of Convergence:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} \\
![definition](/main_files/128a/2/2.4/1.png)

2. **Important, Two cases of order:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} \\
<img src="/main_files/128a/2/2.4/2.png" alt="Ahmad Badary" style="width: 70%;"/>

3. **An arbitrary technique that generates a convergent sequences does so only linearly:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
![definition](/main_files/128a/2/2.4/3.png){: width="85%"}
    > Theorem 2.8 implies that higher-order convergence for fixed-point methods of the form
    > $$ g(p) = p $$ can occur only when $$ g'(p) = 0 $$.

4. **Conditions to ensure Quadratic Convergence:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
![definition](/main_files/128a/2/2.4/4.png){: width="85%"}


5. **Theorems 2.8 and 2.9 imply:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} \\
    (i)\\
    ![definition](/main_files/128a/2/2.4/5.png){:width="90%"} \\
    (ii)\\
    ![definition](/main_files/128a/2/2.4/6.png){:width="90%"}

5. **Newtons' Method Convergence Rate:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} \\
    ![definition](/main_files/128a/2/2.4/6.png){:width="90%"}
    <button>Show Proof</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/2/2.4/derivation1.png){: width="80%" hidden=""}

## Multiple Roots 
{: #content2}

1. **Problem:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} \\
    Newton’s method and the Secant method will generally give
    problems if $$ f'( p) = 0$$ when $$f ( p) = 0 $$.

2. **Zeros and their Multiplicity:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
![definition](/main_files/128a/2/2.4/7.png)

3. **Identifying Simple Zeros:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    ![Thm](/main_files/128a/2/2.4/8.png)
    * **Generalization of Theorem 2.11:**
    ![Thm](/main_files/128a/2/2.4/9.png)

        > The result in Theorem 2.12 implies that an interval about p exists where Newton’s
        > method converges quadratically to p for any initial approximation $$ p_0 = p$$, provided that p
        > is a simple zero.

4. **Why Simple Zeros:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24} \\
    Quadratic convergence might not occur if the zero is not simple
    > Example:
    > Let $$f (x) = e^x − x − 1$$ 
    > Notice that Newton’s method with $$p_0 = 1$$ converges to the zero $$x=0$$ but not quadratically


5. **Handling the problem of multiple roots:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents25} \\
    We Modify Newton's Method \\
    We define $$g(x)$$ as: \\
        ![definition](/main_files/128a/2/2.4/12.png){:height="80px"} 

    [Derivation can be found here!](/main_files/128a/2/2.4/derivation.png)

    * **Properties:**
        * If g has the required continuity conditions, functional iteration applied to $$g$$ will be
        quadratically convergent regardless of the multiplicity of the zero of $$f$$ .
        * Theoretically, the only drawback to this method is the additional calculation of $$f
        (x)$$ and the more laborious procedure of calculating the iterates.
        * In practice, multiple roots can cause serious round-off problems because the denominator of (2.13) consists of the difference of two numbers that are both close to 0.
        * In the case of a simple zero the original Newton’s method requires substantially less computation.

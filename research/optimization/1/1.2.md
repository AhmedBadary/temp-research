---
layout: NotesPage
title: 1.1 | 1.2 <br /> Optimization Models
permalink: /work_files/research/conv_opt/1
prevLink: /work_files/research/conv_opt.html
---
<div markdown="1" class = "TOC">
# Table of Contents

  * [Mathematical Background](#content1)
  {: .TOC1}
  * [Mathematical Formulation [Standard Forms]](#content2)
  {: .TOC2}
  * [Nomenclature](#content3)
  {: .TOC3}
  * [Problem Classes](#content4)
  {: .TOC4}
</div>

***
***

## Mathematical Background
{: #content1}

1. **Maps:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}
    :   We reserve the term map to refer to vector-valued functions. That is, maps are
    functions which return more than a single value.

2. **Graph:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} \\
    Consider a function $$f : \mathbf{R}^n \rightarrow \mathbf{R}$$.  
    The _**graph**_ of $$f$$ is the set of input-output pairs that $$f$$ can attain, that is:
    $$G(f) := \left \{ (x,f(x)) \in \mathbf{R}^{n+1} : x \in \mathbf{R}^n \right \}.$$ \\
    > It is a subset of $$\mathbf{R}^{n+1}$$.

3. **Epigraph:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    Consider a function $$f : \mathbf{R}^n \rightarrow \mathbf{R}$$.  
    The _**epigraph**_, denoted $$\mathop{\bf epi} f$$, describes the set of input-output pairs that $$f$$ can achieve, as well as "anything above":  
    $$\mathop{\bf epi} f := \left \{ (x,t) \in \mathbf{R}^{n+1} ~:~ x \in \mathbf{R}^n, \ \  t \ge f(x) \right \}.$$
    > epi in Greek means "above"  

    <button>Example [image]</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/1/1.1_2/1.png){: hidden=""}

4. **Level and Sub-level Sets:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    > _**Level**_ and _**sub-level**_ sets correspond to the notion of contour of a function. Both are indexed on some scalar value $$t$$.  

    * **Level sets**: is simply the set of points that achieve exactly some value for the function $$f$$.  
    For $$t \in \mathbf{R}$$, the $$t-$$level set of the function $$f$$ is defined as:  
    $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathbf{L}_t(f) := \left\{ x \in \mathbf{R}^{n} ~:~ x \in \mathbf{R}^n, \ \  t = f(x) \right \}.$$

    * **Sub-level sets**: is the set of points that achieve at most a certain value for  $$f$$, or below:  
    $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \mathbf{S}_t(f) := \left\{ x \in \mathbf{R}^{n} ~:~ x \in \mathbf{R}^n, \ \  t \ge f(x) \right\}.$$  

    <button>Example [image]</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/1/1.1_2/2.png){: hidden=""}

***

## Mathematical Formulation [Standard Forms]
{: #content2}

1. **Functional Form:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21}
    :   An optimization problem is a problem of the form
    $$p^\ast := \displaystyle\min_x f_0(x) :  f_i(x) \le 0, \ \  i=1,\ldots, m$$,  
    where:  
        * $$x \in \mathbf{R}^n$$ is the decision variable;

        * $$f_0 : \mathbf{R}^n \rightarrow \mathbf{R}$$ is the objective function, or cost; 

        * $$f_i : \mathbf{R}^n \rightarrow \mathbf{R}, \ \  i=1, \ldots, m$$ represent the constraints;

        * $$p^\ast$$ is the optimal value.  

    * [**Example.**](http://livebooklabs.com/keeppies/c5a5868ce26b8125/e5deae3e0c61b80e){: value="show" onclick="iframePopA(event)"}
        <a href="http://livebooklabs.com/keeppies/c5a5868ce26b8125/e5deae3e0c61b80e">` Visit the Book`</a>
        <div markdown="1"> </div>

2. **Epigraph form:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22}
    :   TODO

3. **Other Standard-Forms:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23}
    :   TODO

***

## Nomenclature
{: #content3}

1. **Feasible set:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31}
    :   $$ \mathbf{X} :=  \left\{ x \in \mathbf{R}^n ~:~  f_i(x) \le 0, \ \  i=1, \ldots, m \right\}.$$  

2. **Solution:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32}
    :   In an optimization problem, we are usually interested in computing the optimal value of the objective function, and also often a minimizer, which is a vector which achieves that value, if any.

3. **Feasibility problems:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} 
    :   Sometimes an objective function is not provided. This means that we are just _interested_ in _finding_ a _feasible point_, or determine that the problem is _infeasible_.  
    > By convention, we set $$f_0$$ to be a constant in that case, to reflect the fact that we are indifferent to the choice of a point x as long as it is feasible.

4. **Optimal value:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents34}
    :    $$p^\ast := \min_x : f_0(x) ~:~ f_i(x) \le 0, \ \  i=1, \ldots, m.$$   

    > Denoted $$p^\ast$$.

5. **Optimal set:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents35} 
    :   The set of feasible points for which the objective function achieves the optimal value:  
    $$ \mathbf{X}^{\rm opt} :=  \left\{ x \in \mathbf{R}^n ~:~  f_0(x) = p^\ast, \ \ f_i(x) \le 0, \ \  i=1,\ldots, m \right\}$$.  
    Equivalently,  
    $$ \mathbf{X}^{\rm opt} = \mathrm{arg min}_{x \in \mathbf{X}}  f_0(x)$$.  

    > We take the convention that the optimal set is empty if the problem is not feasible.  

    > A _point_ $$x$$ is said to be **optimal** if it belongs to the optimal set.

    > If the optimal value is **ONLY** attained in the **limit**, then it is **NOT** in the optimal set.

6. **When is a problem "_Attained_"?**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents36}
    :   If the optimal set is _not empty_, we say that the problem is **_attained_**.

7. **Suboptimality:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents37}
    :   The $$\epsilon$$-suboptimal set is defined as:  
    :   $$ \mathbf{X}_\epsilon := \left\{ x \in \mathbf{R}^n ~:~ f_i(x) \le 0, \ \  i=1, \ldots, m, \ \  f_0(x) \le p^\ast + \epsilon \right\}.$$  

    > $$\implies \  \mathbf{X}_0 = \mathbf{X}_{\rm opt}$$.

8. **(Local and Global) _Optimality_:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents38} \\
    * A point $$z$$ is **Locally Optimal**: if there is a value $$R>0$$ such that $$z$$ is optimal for the following problem:
    $$min_x : f_0(x) ~:~ f_i(x) \le 0, \ \ i=1, \ldots, m,  \ \ \|z-x\|_2 \le R$$.  
    > i.e. a _local minimizer_ $$x$$ _minimizes_ $$f_0$$, but **only** for _nearby points_ on the feasible set.

    * A point $$z$$ is **Globally Optimal**: if it is the _optimal value_ of the original problem on all of the feasible region.   

***

## Problem Classes
{: #content4}

1. **Least-squares:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41}
    :   $$\min_x \;\;\;\; \sum_{i=1}^m \left( \sum_{j=1}^n A_{ij} . x_j - b_i \right)^2,$$
    :   where $$A_{ij}, \  b_i, \  1 \le i \le m,  \ 1 \le j \le n$$, are given numbers, and $$x \in \mathbf{R}^n$$ is the variable.

2. **Linear Programming:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents42} 
    :   $$ \min \sum_{j=1}^n c_jx_j ~:~ \sum_{j=1}^n A_{ij} . x_j  \le b_i , \;\; i=1, \ldots, m, $$ 
    :   where $$ c_j, b_i$$ and $$A_{ij}, \  1 \le i \le m, \  1 \le j \le n$$, are given real numbers.  

    > This corresponds to the case where the functions $$f_i(i=0, \ldots, m)$$ in the standard problem are all affine (that is, linear plus a constant term).  

    > Denoted $$LP$$.

3. **Quadratic Programming:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents43}
    :   $$\min_x \;\;\;\; \displaystyle\sum_{i=1}^m \left(\sum_{j=1}^n C_{ij} . x_j+d_i\right)^2 + \sum_{i=1}^n c_ix_i \;:\;\;\;\; \sum_{j=1}^m A_{ij} . x_j \le b_i, \;\;\;\; i=1,\ldots,m.$$  

    > Includes a **sum of squared linear functions**, in addition to a **linear term**, in the _objective_.  

    > QP's are popular in finance, where the _linear term_ in the _objective_ refers to the _expected negative return_ on an investment, and the _squared terms_ corresponds to the _risk_ (or variance of the return).  

    > QP was introduced by "Markowitz"

4. **Nonlinear optimization:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents44}
    :   A broad class that includes _Combinatorial Optimization_.

    > One of the reasons for which non-linear problems are hard to solve is the issue of _local minima_.

5. **Convex optimization:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents45}
    :    A generalization of QP, where the objective and constraints involve "bowl-shaped", or convex, functions.

    > They are _easy_ to solve because they _do not suffer_ from the "curse" of _local minima_.

6. **Combinatorial optimization:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents46}
    :   In combinatorial optimization, some (or all) the variables are boolean (or integers), reflecting discrete choices to be made.

    > Combinatorial optimization problems are in general extremely hard to solve. Often, they can be approximately solved with linear or convex programming.

7. **NON-Convex Optimization Problems [Examples]:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents47} \\
    * **Boolean/integer optimization:** some variables are constrained to be Boolean or integers.  
    > Convex optimization can be used for getting (sometimes) good approximations.
    * **Cardinality-constrained problems:** we seek to bound the number of non-zero elements in a vector variable.  
    > Convex optimization can be used for getting good approximations.
    * **Non-linear programming:** usually non-convex problems with differentiable objective and functions.  
    > Algorithms provide only local minima.  

    > Most (but not all) non-convex problems are hard!
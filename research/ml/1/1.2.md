---
layout: NotesPage
title: The Perceptron
permalink: /work_files/research/ml/1/2
prevLink: /work_files/research/ml.html
---

<div markdown="1" class = "TOC">
# Table of Contents

  * [Introduction](#content1)
  {: .TOC1}
  * [The Perceptron Method](#content2)
  {: .TOC2}
  * [Convergence and Complexity](#content3)
  {: .TOC3}
  * [Further Analysis](#content4)
  {: .TOC4}
</div>

***
***

* [A smooth Perceptron Algorithm: or how to smooth out a loss function (paper)](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.436.4725&rep=rep1&type=pdf)  



## Introduction
{: #content1}

1. **The Perceptron:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} 
    :   The **Perceptron** is an algorithm for supervised learning of binary classifiers.

2. **Type:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} 
    :   It is a type of linear classifiers.

3. **The Problem Setup:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} 
    :   Consider $$n$$ sample points $$X_1, X_2, ..., X_n$$.
    :   For each sample point, let $${\displaystyle y_i = {\begin{cases} \:\: 1&{\text{if }}\ X_i \in C \\-1&{\text{if}}\ X_i \notin C\end{cases}}}$$
    :   > where 'C' is a given class of interest.

***

## The Perceptron Method
{: #content2}

0. **Goal:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents20} 
    :   Find weights '$$w$$' such that: $${\displaystyle {\begin{cases}X_i \cdot w \geq 0&{\text{if }}\ y_i = 1\\X_i \cdot w \leq 0&{\text{if }}\ y_i = -1\end{cases}}}$$
    :   > Where $$X_i \cdot w$$ is the signed distance.
    :   Equivalently:  
    :   $$y_iX_i \cdot w \geq 0$$
    :   > Where $$y_iX_i \cdot w \geq 0$$ is a constraint on the problem.

1. **Procedure:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} 
    :   Compute the point of greatest descent until you find a local minima and update the weights using "Gradient Descent".

2. **Decision Function:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} 
    :   $${\displaystyle f(x)={\begin{cases}1&{\text{if }}\ w\cdot X_i+\alpha>0\\0&{\text{otherwise}}\end{cases}}}$$
    :   where $$\alpha$$ is added by [The fictitious Diminsion Trick](#bodyContents28)

3. **Loss Function:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} 
    :   $${\displaystyle L(z, y_i) = {\begin{cases}0&{\text{if }}\ y_i\cdot z_i \geq 0\\-y_i z&{\text{otherwise}}\end{cases}}}$$

4. **Objective (cost) Function:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24} 
    :   $$R(w) = \sum_{i=1}^n L(X_i \cdot w, y_i) = \sum_{i \in V} -y_iX_i \cdot w$$
    :   > where $$V$$ is the set of indices $$i$$ for which $$y_iX_i \cdot w < 0$$.  

    * Risk func is __Convex__ but __Non-Smooth__?  

5. **Constraints:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents25} 
    :   $$y_iX_i \cdot w \geq 0$$

00. **The Optimization Problem:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents200} 
    :   Find weights $$w$$ that minimizes $$R(w)$$.

6. **Optimization Methods:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents26}
    :   The Perceptron algorithm uses a _numerical optimization_ method.
    :   **Gradient Descent** is the most commonly used method.
    :   **Newtons Method** can also be used to optimize the objective. 

77. **The Gradient Descent Step:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents277}
    :   $$\begin{align}
            \nabla_w R(w) & \ = \\
            & \ = \nabla_w \sum_{i=1}^n L(X_i \cdot w, y_i) \\
            & \ = \nabla_w \sum_{i \in V} -y_iX_i \cdot w \\
            & \ = \sum_{i \in V} -y_iX_i
            \end{align}$$

7. **The Algorithm (Frank Rosenblatt, 1957):**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents27}
    :   (1) Choose the weights $$\vec{w}$$ arbitrarily.  
    :   (2) While $$R(\vec{w}) > 0$$:  \\
        $$\:\:\:\:\:\:\:$$ (3) $$V \leftarrow$$ set of indices such that: $$ y_iX_i \cdot w < 0$$  
        $$\:\:\:\:\:\:\:$$ (4) $$ w \leftarrow w + \epsilon \cdot \sum_{i \in V} y_iX_i \;\;\;\;\;\; $$ [GD]  
        $$\:\:\:\:\:\:\:$$ (4) $$ w \leftarrow w + \epsilon \cdot y_iX_i \;\;\;\;\;\; $$ [SGD]  
    :   (5) Recurse

8. **Avoiding the constriction of the separating hyperplane to passing through the origin:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents28}   
    :   In the procedure we have just described, the separating hyperplane that this algorithm will produce will be forced to pass through the origin, since the resulting hyperplane is not translated from the origin.
    :   We can get around that by moving our problem to a higher diminsion.  
        We achieve that by adding a "fictitious" diminsion as follows:
    :   We re-write,  
    :   $$\vec{w}\cdot X_i \rightarrow \left(\begin{array}{c} w_1  & w_2  & \cdots & w_d & \alpha  \end{array} \right) \cdot \left(\begin{array}{ccccc} x_1  \\ x_2 \\ \vdots \\ x_d\\ 1 \end{array} \right)$$
    :   Now, we run the perceptron algorithm in (d + 1)-dimensional space.

12. **The Boundary:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents212}  
    :   The boundary is a hyperplane:  
    :   $$\{x \in \mathbf{R}^d : f(x) = 0\}$$  
    :   > where $$f(x) = wX_i + \alpha$$.


***

## Convergence and Complexity
{: #content3}

1. **The Perceptron Convergence Theorem I:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31} 
    :   If the data is linearly separable, the perceptron algorithm will always find  a linear classifier that classifies all the data points correctly.

2. **The Perceptron Convergence Theorem II:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32} 
    :   If the perceptron is guranteed to converge on a data set, then it will converge in at most $$\mathcal{O}(\dfrac{R^2}{\gamma})$$ iterations.
    :   > where $$R = \max_i \|X_i\|$$, called the _radius_ of the data, and $$\gamma = $$ max margin possible.

3. **Complexity (Runtime):**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} 
    :   $$\mathcal{O}(n*d)$$
    :   Can be made faster with 'SGD'.
    :   Although the step size/learning rate doesnâ€™t appear in that big-O expression, it does have an effect on the
    running time, but the effect is hard to characterize.  
        The algorithm gets slower if $$\epsilon$$ is too small because it has to take lots of steps to get down the hill. But it also gets slower if $$\epsilon$$ is too big for a different reason: it jumps
        right over the region with zero risk and oscillates back and forth for a long time.

***

## Further Analysis
{: #content4}

1. **Properties:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41} 
    1. It is an _Online_ Algorithm.
    2. The algorithm is quite slow.
    3. There is no way to reliably choose the learning rate.
    4. It is currently obsolete.
    5. It will not converge, nor approach any approximate solutions for non-linearly separable data.  

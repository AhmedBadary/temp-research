---
layout: NotesPage
title: Hard-Margin Support Vector Machines <br /> SVM
permalink: /work_files/research/ml/1/4
prevLink: /work_files/research/ml.html
---

<div markdown="1" class = "TOC">
# Table of Contents

  * [Introduction - Support Vector Machines](#content1)
  {: .TOC1}
  * [The Hard-Margin SVM](#content2)
  {: .TOC2}
  * [Further Analysis](#content3)
  {: .TOC3}
</div>

***
***

## Introduction - Support Vector Machines
{: #content1}

1. **Support Vector Machines:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}  
    **Support Vector Machines** (SVMs) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis.  
    The SVM is a [_Maximum Margin Classifier_](/work_files/research/ml/1/3) that aims to find the "maximum-margin hyperplane" that divides the group of points $${\displaystyle {\vec {x}}_{i}} {\vec {x}}_{i}$$ for which $${\displaystyle y_{i}=1}$$ from the group of points for which $${\displaystyle y_{i}=-1}$$.  

3. **Support Vectors:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13}  
    **Support Vectors** are the data-points that lie exactly on the margin (i.e. on the boundary of the slab).  
    They satisfy $$\|w^TX' + b\| = 1, \forall $$ support vectors $$X'$$  

2. **The Hard-Margin SVM:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12}  
    The _Hard-Margin SVM_ is just a maximum-margin classifier with features and kernels (discussed later).  

***

## The Hard-Margin SVM
{: #content2}

0. **Goal:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents20}   
    :   Find weights '$$w$$' and scalar '$$b$$' that correctly classifies the data-points and, moreover, does so in the "_best_" possible way.
    :   > Where we had defined _best_ as the classifier that admits the maximum

1. **Procedure:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21}   
    (1) Use a linear classifier
    (2) But, Maximize the Margin
    (3) Do so by Minimizing $$\|w\|$$  

2. **Decision Function:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22}   
    :   $${\displaystyle f(x)={\begin{cases}1&{\text{if }}\ w\cdot X_i+\alpha>0\\0&{\text{otherwise}}\end{cases}}}$$

5. **Constraints:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents25}   
    :   $$y_i(wX_i + b) \geq 1, \forall i \in [1,n]$$

7. **The Optimization Problem:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents27}    
    Find weights '$$w$$' and scalar '$$b$$' that minimize  
    <p>$$ \dfrac{1}{2} w^Tw$$</p>  
    Subject to  
    <p>$$y_i(wX_i + b) \geq 1, \forall i \in [1,n]$$</p>  
    Formally,  
    <p>$$\min_w \dfrac{1}{2}w^Tw \:\:\: : \:\: y_i(wX_i + b) \geq 1, \forall i \in [1,n]$$</p>  

6. **Optimization Methods:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents26}  
    :   The SVM optimization problem reduces to a [Quadratic Program](work_files/research/conv_opt/3_3).

## Further Analysis
{: #content3}

1. **Generalization:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31}   
    :   We notice that, geometrically, the hyperplane (the maximum margin classifier) is completely characterized by the _support vectors_ (the vectors that lie on the margin).  
    :   A very important conclusion arises.  
        The maximum margin classifier (SVM) depends **only** on the number of support vectors and **_not_** on the diminsion of the problem.  
        This implies that the computation doesn't scale up with the diminsion and, also implies, that the _kernel trick_ works very well.

2. **Properties:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32}   
    :   1. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
        2. The hyperplane is determined solely by its support vectors.
        3. The SVM always converges on linearly seprable data.
        4. The Hard-Margin SVM fails if the data is not linearly separable. 
        4. The Hard-Margin SVM is quite sensetive to outliers
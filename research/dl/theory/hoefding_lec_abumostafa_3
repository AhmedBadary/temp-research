So let's translate this into the learning situation.
Here are your coins.
And how do they correspond to the bins?

Well, it's a binary experiment, whether you are picking a red marble or a green marble, or you are flipping a coin getting heads or tails.
It's a binary situation.  So there's a direct correspondence.
Just get the probability of heads being mu, which is the probability of a red marble, corresponding to them.

Since the coins are fair, actually all the bins in this case are half red, half green.
That's really bad news for a hypothesis.
The hypothesis is completely random. Half the time it agrees with the target function. Half the time it disagrees. No information at all.

Now you apply the learning paradigm we mentioned, and you say: let me generate a sample from the first hypothesis.
I get this, I look at it, and I don't like that.

It has some reds.
I want really a clean hypothesis that performs perfectly--all green.
You move on. And, OK.  
This one is even worse.
You go on and on and on.
And eventually, lo and behold, I have all greens.
Bingo.
I have the perfect hypothesis.

I am going to report this to my customer, and if my customer is in financial forecasting, we are going to beat the stock market and make a lot of money.  And you start thinking about the car you are going to buy, and all of that.
Well, is it bingo?

No, it isn't.  And that is the problem.
So now, we have to find something that makes us deal with multiple bins properly.
Hoeffding Inequality-- if you have one experiment, it has a guarantee.
The guarantee gets terribly diluted as you go, and we want to know exactly how the dilution goes.


So here is a simple solution.
This is a mathematical slide. I'll do it step-by-step.

There is absolutely nothing mysterious about it.
This is the quantity we've been talking about. This is the probability of a bad event.
But in this case, you realize that I'm putting g.
Remember, g was our final hypothesis.
So this corresponds to a process where you had a bunch of h's, and you picked one according to a criterion, that happens to be an in-sample criterion, minimizing the error there, and then you report the g as the one that you chose.
And you would like to make a statement that the probability for the g you chose-- the in-sample error-- happens to be close to the out-of-sample error.
So you'd like the probability of the deviation being bigger than your tolerance to be, again, small.
All we need to do is find a Hoeffding counterpart to this, because now this fellow is loaded.
It's not just a fixed hypothesis and a fixed bin.  It actually corresponds to a large number of bins, and I am visiting the random samples in order to pick one.
So clearly the assumptions of Hoeffding don't apply-- that correspond to a single bin.
This probability is less than or equal to the probability of the following. > 

I have M hypotheses--capital M hypotheses: h_1, h_2, h_3, h_M.
That's my entire learning model.
That's the hypothesis set that I have, finite as I said I would assume.
If you look at what is the probability that the hypothesis you pick is bad? Well, this will be less than or equal to the probability that the first hypothesis is bad, > or the second hypothesis is bad, or, or, or the last hypothesis is bad.

That is obvious.  g is one of them.  If it's bad, one of them is bad.  So less than or equal to that.
This is called the union bound in probability.
It's a very loose bound, in general, because it doesn't consider the overlap.
Remember when I told you that the half a percent here, half a percent here, half a percent here-- if you are very unlucky and these are non-overlapping, they add up.
The non-overlapping is the worst-case assumption, and it is the assumption used by the union bound.
So you get this.
And the good news about this is that I have a handle on each term of them.
The union bound is coming up. 
So I put the OR's.
And then I use the union bound to say that this is > less than or equal to, 

and simply sum the individual probabilities.
So the half a percent plus half a percent plus half a percent--this will be an upper bound on all of them.
The probability that one of them goes wrong, the probability that someone gets all heads, and I add the probability for all of you, and that makes it a respectable probability.
So this event here is implied. Therefore, I have the implication because of the OR, and this one because of the union bound, where I have the pessimistic assumption that I just need to add the probabilities.
Now, all of this-- again, we make simplistic assumptions, which is really not simplistic as in trivially restricting, but rather the opposite.
We just don't want to make any assumptions that restrict the applicability of our result. So we took the worst case.


Well, if you look at this, now I have good news for you.
Because each term here is a fixed hypothesis.  I didn't choose anything.
Every one of them has a hypothesis that was declared ahead of time.
Every one of them is a bin.
So if I look at a term by itself, Hoeffding applies to this, exactly the same way it applied before.
So this is a true mathematical statement now.
- I'm not looking at the bigger experiment.
    I reduced the bigger experiment to a bunch of quantities.
    Each of them corresponds to a simple experiment that we already solved.
    So I can substitute for each of these with the bound that the Hoeffding gives me. >

So what is the bound that the Hoeffding gives me?
That's the one.
For every one of the hs, each of these guys was less than or equal to this quantity.  One by one. All of them are obviously the same.  So each of them is smaller than this quantity.
Now I can be confident that the probability that I'm interested in, which is the probability that the in-sample error being close to the out-of-sample error-- the closeness of them is bigger than my tolerance, the bad event.
Under the genuine learning scenario-- you generate marbles from every bin, and you look deliberately for a sample that happens to be all green or as green as possible, and you pick this one.
And you want an assurance that whatever that might be, the corresponding bin will genuinely be good out-of-sample.
That is what is captured by this probability.
That is still bounded by something, which also has that exponential in it, which is good.
But it has an added factor that will be a very bothersome factor, which is: I have > capital M of them.

Now, this is the bad event.  I'd like the probability to be small.
I don't like to magnify the right-hand side, because that is the probability of something bad happening.
Now, with M, you realize that if you use 10 hypotheses, this probability is probably tight.
If you use a million hypotheses, we probably are already in trouble.
There is no guarantee, because now the million gets multiplied by what used to be a respectable probability, which is 1 in 100,000, and now you can make the statement that the probability that something bad happens is less than 10.
Yeah, thank you very much. We have to take a graduate course to learn that! Haha

Now you see what the problem is.
And the problem is extremely intuitive.

We all know/heard that if u use a more sophisticated model, the chances are you will memorize in-sample, and you are not going to really generalize well out-of-sample, because you have so many parameters to work with.
There are so many ways to look at that intuitively, and this is one of them.
If you have a very sophisticated model-- M is huge, let alone infinite.  That's later to come.
That's what the theory of generalization is about.
But if you pick a very sophisticated example with a large M, you lose the link between the in-sample and the out-of-sample.

So this fellow is supposed to track this fellow.
The in-sample is supposed to track the out-of-sample.
The more sophisticated the model you use, the looser that in-sample will track the out-of-sample.
Because the probability of them deviating becomes bigger and bigger and bigger.
And that is exactly the intuition we have.
Now, surprise.
